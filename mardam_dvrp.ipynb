{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Đọc file CSV\n",
    "file_path = \"F:/KLTN/100_Customer/h100c101.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Chuyển đổi dữ liệu thành numpy array\n",
    "locations = df[['x', 'y']].values\n",
    "demand = df['demand'].values\n",
    "open_time = df['open'].values\n",
    "close_time = df['close'].values\n",
    "service_time = df['servicetime'].values\n",
    "arrival_time = df['time'].values\n",
    "\n",
    "# Chuẩn hóa tọa độ (nếu cần)\n",
    "locations = locations / np.max(locations, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class DVRPEnv(gym.Env):\n",
    "    def __init__(self, df, max_capacity=1300):\n",
    "        super(DVRPEnv, self).__init__()\n",
    "\n",
    "        self.df = df\n",
    "        self.max_capacity = max_capacity\n",
    "        self.current_time = 0\n",
    "        self.vehicle_capacity = max_capacity\n",
    "        self.current_position = np.array([0, 0])  # Xe xuất phát từ (0,0)\n",
    "        \n",
    "        # Định nghĩa không gian hành động và trạng thái\n",
    "        self.action_space = spaces.Discrete(len(df))  # Chọn 1 khách hàng để phục vụ\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(len(df), 5), dtype=np.float32)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_time = 0\n",
    "        self.vehicle_capacity = self.max_capacity\n",
    "        self.current_position = np.array([0, 0])\n",
    "        self.done = False\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        return np.hstack((self.df[['x', 'y', 'demand', 'open', 'close']].values, \n",
    "                      np.full((len(self.df), 1), self.current_time)))  # Thêm cột time\n",
    "\n",
    "    def step(self, action):\n",
    "        if action < 0 or action >= len(self.df):\n",
    "            raise ValueError(f\"Invalid action index: {action}. Must be between 0 and {len(self.df)-1}\")\n",
    "        \n",
    "        selected = self.df.iloc[action]\n",
    "        distance = np.linalg.norm(self.current_position - np.array([selected['x'], selected['y']]))\n",
    "        \n",
    "        if self.vehicle_capacity < selected['demand'] or self.current_time > selected['close']:\n",
    "            reward = -10  # hoặc giá trị phạt hợp lý\n",
    "            done = True\n",
    "        else:\n",
    "            reward = 10  # hoặc giá trị thưởng hợp lý\n",
    "            done = False\n",
    "\n",
    "        return self._get_observation(), reward, done, {}\n",
    "\n",
    "\n",
    "env = DVRPEnv(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MARDAMModel(nn.Module):\n",
    "    def __init__(self, input_dim=6, hidden_dim=128, output_dim=10):  # input_dim=7\n",
    "        super(MARDAMModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # Ánh xạ từ 7 → 128\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = MARDAMModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid action index: 1005. Must be between 0 and 100",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m action_values \u001b[38;5;241m=\u001b[39m model(state_tensor)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m     14\u001b[0m action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(action_values)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 16\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(action_values[action], torch\u001b[38;5;241m.\u001b[39mtensor(reward, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[1;32mIn[21], line 33\u001b[0m, in \u001b[0;36mDVRPEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m action \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf):\n\u001b[1;32m---> 33\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid action index: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Must be between 0 and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m     selected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[action]\n\u001b[0;32m     36\u001b[0m     distance \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_position \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39marray([selected[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m], selected[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]]))\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid action index: 1005. Must be between 0 and 100"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for episode in range(500):  # Số lần huấn luyện\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        action_values = model(state_tensor).squeeze()\n",
    "        action = torch.argmax(action_values).item()\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        loss = loss_fn(action_values[action], torch.tensor(reward, dtype=torch.float32))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode {episode}, Total Reward: {total_reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
